{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea85a120-4dc4-42ca-991c-84c58b6268c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run this cell\n",
    "# !git clone https://github.com/mikedataCrunch/GMS5204.git\n",
    "# !mv ./GMS5204/* .\n",
    "# !rm ./GMS5204"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598dc474-4c10-4759-b86d-9d9e9dc161e4",
   "metadata": {},
   "source": [
    "# Curve fitting - Artificial Neural Networks\n",
    "\n",
    "In this activity, we will use a simple artificial neural network to fit a model the provides the best decision boundary to the data provided. At this point of the lecture, you will learn that while traditional curve fitting might struggle with non-linear relationships or interactions between multiple variables, neural networks excel in these areas due to their layered architecture and non-linear activation functions. Essentially, a neural network can be thought of as performing a highly sophisticated form of curve fitting. It adjusts its internal parameters (weights and biases) during training to minimize the loss function, analogous to how coefficients are adjusted in polynomial curve fitting. This enables neural networks to fit intricate patterns in data, making them powerful tools for tasks that involve image recognition, natural language processing, and more, where traditional models might fail to capture the complexity of the data.\n",
    "\n",
    "This notebook assists learning the advantages of combinining non-linear activations with a more complex model architecture i.e., a neural network.\n",
    "\n",
    "## Neural network architecture\n",
    "\n",
    "The following illustration is our model architecture. It is a simple neural network with two inputs (i.e., `Feature 0` and `Feature 1`), a single `hidden` layer with two `nodes` or `units`, and an output layer with a single `node`. In binary classification tasks, meaning, those that answer \"Yes\"/\"No\", \"Positive\"/\"Negative\", and \"Class 0\" vs \"Class 1\" questions, a single output node will do. The values here are then thresholded, meaning we set a value decision boundary and anything north or south of that value are mapped to the two classes of outputs. The labels in the illustration should guide you when changing the slider values for each parameter in the network.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./media/nn_activity.jpeg\"/>\n",
    "</center>\n",
    "<center><b>Figure 1. Simple neural network architecture: 2 input features, 1 hidden layer with 2 nodes, and an output layer with a single node.</b></center>\n",
    "\n",
    "## Data description\n",
    "The sample data we're using here resembles a binary classification, where each sample belongs to either `Class: 0` or `Class: 1`. This is quite common in tasks that requires identifying examples that are `positive` to a particular condition, disease, diagnosis, or some other classification criteria.\n",
    "\n",
    "The binary classification task takes in two input `features`. We can think of features as characteristics of an example. If consider humans as examples, then the two features can be height & weight, age & gender, gender & income, etc. If we consider this in a medical sense, then the two features can be test result A & B, lifestyle & age, or whatever pair of characteristics available to us.\n",
    "\n",
    "In our activity, we will refer to these as `Feature 0` and `Feature 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "028f7037-0ad8-42ed-81a5-8b97e21c941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive, FloatSlider, Layout\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def ReLU(x):\n",
    "    \"\"\"ReLU: Rectified linear unit function.\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Define the neural network with one hidden layer\n",
    "def simple_neural_network(inputs, w1, w2, b1, b2):\n",
    "    # Inputs is expected to be Nx2, w1 is 2x2, b1 is size 2, w2 is size 2, b2 is a scalar\n",
    "    hidden_layer_input = np.dot(inputs, w1) + b1\n",
    "    hidden_layer_activation = ReLU(hidden_layer_input) \n",
    "    output = np.dot(hidden_layer_activation, w2) + b2\n",
    "    return 1 / (1 + np.exp(-output))  # Sigmoid activation for output layer or output activation\n",
    "\n",
    "def calculate_bce_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the binary cross-entropy loss.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true (array-like): True binary labels (0 or 1).\n",
    "    y_pred (array-like): Predicted probabilities, between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float: The average binary cross-entropy loss.\n",
    "    \"\"\"\n",
    "    # Ensure that y_pred does not contain values exactly equal to 0 or 1,\n",
    "    # as log(0) is undefined and can cause computation errors.\n",
    "    epsilon = 1e-10\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "    # Calculate binary cross-entropy loss\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e740f805-a213-462e-9c0f-11bfd97d9d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simple dataset\n",
    "np.random.seed(42)\n",
    "# Class 0\n",
    "feature_0_class_0 = np.random.normal(2, 1, 100)  # Feature 1 for class 0\n",
    "feature_1_class_0 = np.random.normal(2, 1, 100)  # Feature 2 for class 0\n",
    "# Class 1\n",
    "feature_0_class_1 = np.random.normal(5, 1, 100)  # Feature 1 for class 1\n",
    "feature_1_class_1 = np.random.normal(5, 1, 100)  # Feature 2 for class 1\n",
    "\n",
    "features = np.vstack((np.column_stack((feature_0_class_0, feature_1_class_0)),\n",
    "                      np.column_stack((feature_0_class_1, feature_1_class_1))))\n",
    "y_true = np.array([0]*100 + [1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbbf7137-594a-43c1-ac83-aeed10e0b877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce0ac5c80e941988b934bf93aabd343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='w_x0_h0', layout=Layout(width='600px'), max=15.0, miâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting function\n",
    "def plot_nn_decision_boundary(w1_00, w1_01, w1_10, w1_11, b1_0, b1_1, w2_0, w2_1, b2):\n",
    "    w1 = np.array([[w1_00, w1_01], [w1_10, w1_11]])\n",
    "    b1 = np.array([b1_0, b1_1])\n",
    "    w2 = np.array([w2_0, w2_1])\n",
    "    b2 = np.array([b2])\n",
    "\n",
    "    # offset = 10\n",
    "    offset = 0\n",
    "    # Grid for decision boundary visualization\n",
    "    feature_0, feature_1 = np.meshgrid(\n",
    "        np.linspace(\n",
    "            min(np.concatenate([feature_0_class_0, feature_0_class_1])) - offset, \n",
    "            max(np.concatenate([feature_0_class_0, feature_0_class_1])) + offset, \n",
    "            50\n",
    "        ),\n",
    "        np.linspace(\n",
    "            min(np.concatenate([feature_1_class_0, feature_1_class_1])) - offset, \n",
    "            max(np.concatenate([feature_1_class_0, feature_1_class_1])) + offset, \n",
    "            50\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    zz = simple_neural_network(np.c_[feature_0.ravel(), feature_1.ravel()], w1, w2, b1, b2)\n",
    "    zz = zz.reshape(feature_1.shape)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(\n",
    "        features[:,0][:y_true.size // 2], \n",
    "        features[:,1][:y_true.size // 2], \n",
    "        c='blue',\n",
    "        label='Class: 0',\n",
    "        alpha=0.8\n",
    "    )\n",
    "    plt.scatter(\n",
    "        features[:,0][y_true.size // 2:], \n",
    "        features[:,1][y_true.size // 2:], \n",
    "        c='red',\n",
    "        label='Class: 1',\n",
    "        alpha=0.8\n",
    "    )  \n",
    "    \n",
    "    contour = plt.contourf(\n",
    "        feature_0, \n",
    "        feature_1, \n",
    "        zz, \n",
    "        levels=[0, 0.5, 1], \n",
    "        alpha=0.3, \n",
    "        cmap=\"coolwarm\",\n",
    "    )\n",
    "    plt.colorbar(contour)\n",
    "    # Decision boundary line for the zz = 0.5 threshold\n",
    "    plt.contour(\n",
    "        feature_0, \n",
    "        feature_1, \n",
    "        zz, \n",
    "        levels=[0.5],\n",
    "        colors='k', \n",
    "        vmin=0, \n",
    "        vmax=1, \n",
    "        linestyles='dashed')\n",
    "\n",
    "    # Calculate loss\n",
    "    y_pred = simple_neural_network(np.c_[features[:,0].ravel(), features[:,1].ravel()], w1, w2, b1, b2)\n",
    "    loss = calculate_bce_loss(y_true, y_pred)\n",
    "    \n",
    "    plt.annotate(f\"Current BCE Loss: {loss:.4f}\", xy=(5,2), fontsize=10)\n",
    "    plt.title('Neural Network Decision Boundary')\n",
    "    plt.xlabel('Feature 0')\n",
    "    plt.ylabel('Feature 1')\n",
    "    plt.legend()    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "slider_style = {'description_width': 'initial', 'handle_color': 'lightblue'}  # Adjust handle color and description width\n",
    "layout = Layout(width='600px')\n",
    "\n",
    "# slider range\n",
    "min_ = -15\n",
    "max_ = 15\n",
    "\n",
    "# Create interactive controls\n",
    "interactive_plot = interactive(\n",
    "    plot_nn_decision_boundary,\n",
    "    w1_00=FloatSlider(description=\"w_x0_h0\", value=1, min=min_, max=max_, step=0.01, style=slider_style, layout=layout,),\n",
    "    w1_01=FloatSlider(description=\"w_x0_h1\", value=1, min=min_, max=max_, step=0.01, style=slider_style, layout=layout,),\n",
    "    w1_10=FloatSlider(description=\"w_x1_h0\", value=1, min=min_, max=max_, step=0.01, style=slider_style, layout=layout,),\n",
    "    w1_11=FloatSlider(description=\"w_x1_h1\", value=1, min=min_, max=max_, step=0.01, style=slider_style, layout=layout,),\n",
    "    b1_0=FloatSlider(description=\"b_h0\", value=-1, min=min_, max=max_, step=0.01, style=slider_style, layout=layout,),\n",
    "    b1_1=FloatSlider(description=\"b_h1\", value=-1, min=min_, max=max_, step=0.01, style=slider_style, layout=layout,),\n",
    "    w2_0=FloatSlider(description=\"w_h0_out\", value=1, min=min_, max=max_, step=0.01, style=slider_style, layout=layout,),\n",
    "    w2_1=FloatSlider(description=\"w_h1_out\", value=1, min=min_, max=max_, step=0.01, style=slider_style, layout=layout,),\n",
    "    b2=FloatSlider(description=\"b_out\", value=1, min=min_, max=max_, step=0.01, style=slider_style, layout=layout,)\n",
    ")\n",
    "display(interactive_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cb7d85-2dab-4e99-940d-eb6d7e90e5ad",
   "metadata": {},
   "source": [
    "## End."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db54d5-5951-44b5-a745-7164a839bfe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-env",
   "language": "python",
   "name": "deep-learning-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0dddd2-0d03-4340-b44d-83d87cbccfe8",
   "metadata": {},
   "source": [
    "# Curve Fitting\n",
    "\n",
    "Traditional curve fitting involves using mathematical models, typically polynomial functions, to best approximate the underlying relationship between variables in a dataset. This process adjusts the parameters of the chosen model to minimize the difference—often measured by the sum of the squares of the errors—between the predicted values by the model and the observed data points. This technique is widely used in fields ranging from physics to finance, wherever a relationship between variables needs to be quantified and predictions are required based on observed trends.\n",
    "\n",
    "Since neural networks extend this concept to more complex and high-dimensional data, we shall begin by a refresher on traditional models that are used in curve fitting. These are linear models. This notebook explores such models in both regression and binary classification tasks i.e., through the logistics regression formulation of a linear model. \n",
    "\n",
    "## Data descriptions\n",
    "\n",
    "#### Linear regression examples\n",
    "Synthetic data that resembles a linear behavior--distributed nicely along a line (or in the case of the higher dimensional 2nd example, a plane) was generated to allow learners to **tune** the param\n",
    "\n",
    "#### Logistic regression exercise\n",
    "The sample data we're using here resembles a binary classification, where each sample belongs to either `Class: 0` or `Class: 1`. This is quite common in tasks that requires identifying examples that are `positive` to a particular condition, disease, diagnosis, or some other classification criteria.\n",
    "\n",
    "The binary classification task takes in two input `features`. We can think of features as characteristics of an example. If consider humans as examples, then the two features can be height & weight, age & gender, gender & income, etc. If we consider this in a medical sense, then the two features can be test result A & B, lifestyle & age, or whatever pair of characteristics available to us.\n",
    "\n",
    "In our activity, we will refer to these as `Feature 0` and `Feature 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6df8f9b5-dcc9-4f0b-9ee5-5e65da0de56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run this cell\n",
    "# !git clone https://github.com/mikedataCrunch/GMS5204.git\n",
    "# !mv ./GMS5204/* .\n",
    "# !rm ./GMS5204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8bca477-218c-48c6-b78d-258b2a5f735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive, FloatSlider, Layout, widgets\n",
    "from IPython.display import display\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import  numpy as np\n",
    "\n",
    "\n",
    "def create_line(slope, intercept):\n",
    "    \"\"\"Return x and y arrays that express a line with slope and intercept.\"\"\"\n",
    "    x_vals = np.array([min(x), max(x)])\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    return x_vals, y_vals\n",
    "\n",
    "\n",
    "# Define the model function for the surface\n",
    "def model_surface(x, y, a, b, bias):\n",
    "    return a * x + b * y + bias\n",
    "    \n",
    "def calculate_bce_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the binary cross-entropy loss.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true (array-like): True binary labels (0 or 1).\n",
    "    y_pred (array-like): Predicted probabilities, between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float: The average binary cross-entropy loss.\n",
    "    \"\"\"\n",
    "    # Ensure that y_pred does not contain values exactly equal to 0 or 1,\n",
    "    # as log(0) is undefined and can cause computation errors.\n",
    "    epsilon = 1e-10\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "    # Calculate binary cross-entropy loss\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba12d6bb-ebb2-4153-ba6a-5484b65aaa8b",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d0106ee-7694-451e-9b46-b31a4f44ae83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9032c401e02462abb1c54474dc387d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-2.0, description='Slope:', layout=Layout(width='600px'), max=2.5, min…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate some sample data\n",
    "np.random.seed(42)\n",
    "x_pos = np.random.rand(500) * 10  # random data points between 0 and 10\n",
    "x_neg = np.random.rand(500) * -10  # random data points between 0 and -10\n",
    "x = np.concatenate([x_pos, x_neg])\n",
    "y = 2 * x + 1 + np.random.randn(1000) * 2  # model with noise\n",
    "\n",
    "\n",
    "# Function to update the plot with the new line\n",
    "def update_plot(slope, intercept):\n",
    "    x_vals, y_vals = create_line(slope, intercept)\n",
    "    plt.scatter(x, y, color='blue', label='Data Points', alpha=0.5)\n",
    "    plt.plot(x_vals, y_vals, color='red', label='Fit Line')\n",
    "    plt.title('Interactive Linear Regression')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "slider_style = {'description_width': 'initial', 'handle_color': 'lightblue'}  # Adjust handle color and description width\n",
    "layout = Layout(width='600px')\n",
    "# Set up interactive widget\n",
    "interactive_plot = interactive(\n",
    "    update_plot,\n",
    "    slope=FloatSlider(value=-2.0, min=-2.5, max=2.5, step=0.05, style=slider_style, layout=layout, description='Slope:'),\n",
    "    intercept=FloatSlider(value=-10, min=-10, max=10, step=0.1, style=slider_style, layout=layout, description='Intercept:')\n",
    ")\n",
    "display(interactive_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec66a805-f821-4913-9ca7-1736c1661de7",
   "metadata": {},
   "source": [
    "## Linear Regression with 2 input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c16dab4e-444b-453a-a7ba-0927dbcd9d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147db26f7b8c4f2a8933d7010a418f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='Coeff. a:', layout=Layout(width='600px'), max=2.0, s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the grid for the surface plot\n",
    "x = np.linspace(-10, 10, 20)\n",
    "y = np.linspace(-10, 10, 20)\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "# Generate simulated data points\n",
    "np.random.seed(42)\n",
    "x_points = np.random.uniform(-10, 10, 100)\n",
    "y_points = np.random.uniform(-10, 10, 100)\n",
    "z_points = model_surface(x_points, y_points, 1, 1, 0)  # Initial data points on the surface\n",
    "\n",
    "def plot_surface(a, b, bias, azimuth, elevation):\n",
    "    z = model_surface(x, y, a, b, bias)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plotting the surface\n",
    "    ax.plot_surface(x, y, z, cmap='viridis', edgecolor='none', alpha=0.6)\n",
    "    \n",
    "    # Adding scatter plot for the data points\n",
    "    ax.scatter(x_points, y_points, z_points, color='red', label='Data Points')\n",
    "    \n",
    "    # Setting labels and title\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_zlabel('Z (Output)')\n",
    "    ax.set_title('Interactive 3D Surface Plot')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Set the view angle\n",
    "    ax.view_init(elev=elevation, azim=azimuth)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "slider_style = {'description_width': 'initial', 'handle_color': 'lightblue'}  # Adjust handle color and description width\n",
    "layout = Layout(width='600px')\n",
    "\n",
    "# Create interactive widget\n",
    "interactive_plot = interactive(\n",
    "    plot_surface,\n",
    "    a=FloatSlider(value=1, min=0, max=2, step=0.1, description='Coeff. a:', style=slider_style, layout=layout),\n",
    "    b=FloatSlider(value=1, min=0, max=2, step=0.1, description='Coeff. b:', style=slider_style, layout=layout),\n",
    "    bias=FloatSlider(value=0, min=-5, max=5, step=0.1, description='Bias:', style=slider_style, layout=layout),\n",
    "    azimuth=FloatSlider(value=45, min=0, max=360, step=1, description='Orient: z:', style=slider_style, layout=layout),\n",
    "    elevation=FloatSlider(value=30, min=0, max=90, step=1, description='Orient: elev:', style=slider_style, layout=layout)\n",
    "                              \n",
    ")\n",
    "display(interactive_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcda31ec-9ba6-47a9-bfab-4724a7102aac",
   "metadata": {},
   "source": [
    "## Class Activity: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28bd77d4-6f39-4571-af22-9f1a8ef313f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic regression function\n",
    "def logistic_regression(feature_1, feature_2, weights, bias):\n",
    "    logit = (weights[0] * feature_1) + (weights[1] * feature_2) + bias\n",
    "    return 1 / (1 + np.exp(-logit)) # sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7feea8c2-ce4c-4305-a408-5642ebdcc15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "# Class 0\n",
    "feature_0_class_0 = np.random.normal(2, 1, 100)  # Feature 1 for class 0\n",
    "feature_1_class_0 = np.random.normal(2, 1, 100)  # Feature 2 for class 0\n",
    "# Class 1\n",
    "feature_0_class_1 = np.random.normal(5, 1, 100)  # Feature 1 for class 1\n",
    "feature_1_class_1 = np.random.normal(5, 1, 100)  # Feature 2 for class 1\n",
    "\n",
    "features = np.vstack((np.column_stack((feature_0_class_0, feature_1_class_0)),\n",
    "                      np.column_stack((feature_0_class_1, feature_1_class_1))))\n",
    "y_true = np.array([0]*100 + [1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e148d685-d8c6-4697-9ad2-d1e7be349f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true[:y_true.size // 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f80dfc3-5ea1-446a-bb9f-8c3baaf02fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084a4f1fabf0465eb072e34e30ca7cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='Weight 1:', layout=Layout(width='600px'), max=15.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_decision_boundary(weight1, weight2, bias):\n",
    "    # Grid for decision boundary visualization\n",
    "    feature_0, feature_1 = np.meshgrid(\n",
    "        np.linspace(\n",
    "            min(np.concatenate([feature_0_class_0, feature_0_class_1])), \n",
    "            max(np.concatenate([feature_0_class_0, feature_0_class_1])), \n",
    "            50\n",
    "        ),\n",
    "        np.linspace(\n",
    "            min(np.concatenate([feature_1_class_0, feature_1_class_1])), \n",
    "            max(np.concatenate([feature_1_class_0, feature_1_class_1])), \n",
    "            50\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Calculate z values for the contour\n",
    "    zz = logistic_regression(feature_0, feature_1, [weight1, weight2], bias)\n",
    "    zz = zz.reshape(feature_0.shape)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(\n",
    "        features[:,0][:y_true.size // 2], \n",
    "        features[:,1][:y_true.size // 2], \n",
    "        c='blue',\n",
    "        label='Class: 0',\n",
    "        alpha=0.8\n",
    "    )\n",
    "    plt.scatter(\n",
    "        features[:,0][y_true.size // 2:], \n",
    "        features[:,1][y_true.size // 2:], \n",
    "        c='red',\n",
    "        label='Class: 1',\n",
    "        alpha=0.8\n",
    "    )\n",
    "    \n",
    "    contour = plt.contourf(\n",
    "        feature_0, \n",
    "        feature_1,    \n",
    "        zz, \n",
    "        levels=[0, 0.5, 1],\n",
    "        cmap=\"coolwarm\", \n",
    "        alpha=0.3)\n",
    "    \n",
    "    plt.colorbar(contour)\n",
    "    # Decision boundary line for the zz = 0.5 threshold\n",
    "    plt.contour(\n",
    "        feature_0, \n",
    "        feature_1,\n",
    "        zz, \n",
    "        levels=[0.5], \n",
    "        colors='k', \n",
    "        vmin=0, vmax=1, \n",
    "        linestyles='dashed')\n",
    "    \n",
    "    plt.title('Interactive Logistic Regression Decision Boundary')\n",
    "    plt.xlabel('Feature 0')\n",
    "    plt.ylabel('Feature 1')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "slider_style = {'description_width': 'initial', 'handle_color': 'lightblue'}  # Adjust handle color and description width\n",
    "layout = Layout(width='600px')\n",
    "\n",
    "# Create interactive widget\n",
    "interactive_plot = interactive(\n",
    "    plot_decision_boundary,                       \n",
    "    weight1=FloatSlider(value=1, min=-15, max=15, step=0.01, style=slider_style, layout=layout, description='Weight 1:'),                 \n",
    "    weight2=FloatSlider(value=-1, min=-15, max=15, step=0.01, style=slider_style, layout=layout, description='Weight 2:'),       \n",
    "    bias=FloatSlider(value=0, min=-20, max=20, step=0.01, style=slider_style, layout=layout, description='Bias:'))\n",
    "display(interactive_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46984d2a-4c3d-4d57-9f8c-be4e94db7345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea412e6d6b2d46baa9eb15f5f5c1297f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='Weight 1:', step=0.1, style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb82f2bdb164d47ae776f7988337b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='Weight 2:', step=0.1, style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e268a8b67a54ccc8bdf98a0b5cc59d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='Bias', step=0.1, style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43aab97b723d4770bda6f42d3c59accb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weight_1 = widgets.FloatText(\n",
    "    value=0.0,\n",
    "    description='Weight 1:',\n",
    "    step=0.1,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "weight_2 = widgets.FloatText(\n",
    "    value=0.0,\n",
    "    description='Weight 2:',\n",
    "    step=0.1,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "bias = widgets.FloatText(\n",
    "    value=0.0,\n",
    "    description='Bias',\n",
    "    step=0.1,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Display the widget\n",
    "display(weight_1, weight_2, bias)\n",
    "\n",
    "# Output widget to display the results\n",
    "output = widgets.Output()\n",
    "display(output)\n",
    "\n",
    "# Function to update the output based on the inputs\n",
    "def update_output(*args):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        \n",
    "        y_pred = logistic_regression(\n",
    "            features[:, 0], \n",
    "            features[:, 1], \n",
    "            weights=[weight_1.value, weight_2.value], \n",
    "            bias=bias.value\n",
    "        )\n",
    "            \n",
    "        # Calculate loss\n",
    "        loss = calculate_bce_loss(y_true, y_pred)\n",
    "        \n",
    "        print(f\"Weight 1 Value: {weight_1.value}\")\n",
    "        print(f\"Weight 2 Value: {weight_2.value}\")\n",
    "        print(f\"Bias Value: {bias.value}\")\n",
    "\n",
    "        print(f\"BCE LOSS (lower better): {loss:.4f}\")\n",
    "\n",
    "# Attach the observer to the 'value' trait of the float_input widget\n",
    "# Observe changes in each widget and call update_output when any change happens\n",
    "weight_1.observe(update_output, names='value')\n",
    "weight_2.observe(update_output, names='value')\n",
    "bias.observe(update_output, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3f45f-71c5-4377-98d5-7528c2c6285a",
   "metadata": {},
   "source": [
    "## End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-env",
   "language": "python",
   "name": "deep-learning-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
